# Transformer-based Chatbot

A simple Transformer chatbot trained from scratch on the [Blended Skill Talk](https://huggingface.co/datasets/blended_skill_talk) dataset, with support for `word-level` and `SentencePiece BPE tokenizer`.  
This project demonstrates the full workflow of building, training, evaluating, and inferring responses from a transformer model for dialogue generation.

## Project Structure

```text
models/
â”œâ”€â”€ inference.py # Inference entry point
â”œâ”€â”€ save_corpus.py # Export corpus.txt for BPE tokenizer training
â”œâ”€â”€ tokenizer.py # Tokenizer definition (Word-level or SentencePiece)
â”œâ”€â”€ train.py # Training script (Word-level)
â”œâ”€â”€ train_bpe.py # Training script (BPE)
â”œâ”€â”€ transformer_model.py # Transformer architecture definition
bpe.model # Trained SentencePiece model
bpe.vocab # Trained SentencePiece vocab
corpus.txt # Exported corpus used for SentencePiece training
transformer.pth # Saved best model weights
best_accuracy.pkl # Saved best validation accuracy
```

## Features

- End-to-end training on dialogue data  
- Word-level tokenizer and BPE tokenizer support  
- Early stopping and learning rate scheduler  
- Top-k sampling & temperature control during inference  
- Validation loss & accuracy tracking


## ğŸ› ï¸ Requirements

You can install requirements using:

```bash
pip install torch tqdm sentencepiece datasets
```

âš™ï¸ Usage

1ï¸âƒ£ Prepare corpus for BPE training (optional, if you want SentencePiece tokenizer)
```bash
python models/save_corpus.py
```
2ï¸âƒ£ Train SentencePiece tokenizer (optional)
```bash
python models/train_bpe.py
```
3ï¸âƒ£ Train model
Word-level tokenizer:
```bash
python models/train.py
```
BPE tokenizer: Make sure tokenizer.py loads bpe.model correctly.

4ï¸âƒ£ Inference (generate chatbot replies)
```bash
python models/inference.py
```